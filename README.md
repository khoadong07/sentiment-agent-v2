# Sentiment Analysis API v2.0

High-performance production-ready sentiment analysis API vá»›i FastAPI, AI Agent vÃ  Langfuse tracing.

## ğŸš€ Features

- **High Performance**: Multi-instance deployment vá»›i load balancing
- **AI-Powered**: Sá»­ dá»¥ng LLM cho sentiment analysis chÃ­nh xÃ¡c
- **Production Ready**: Redis caching, rate limiting, monitoring
- **Observability**: Langfuse tracing cho AI operations
- **Scalable**: Docker-based deployment vá»›i auto-scaling
- **Monitoring**: Prometheus metrics, health checks

## ğŸ“‹ Requirements

- Docker & Docker Compose
- Python 3.9+
- Redis (optional, cÃ³ fallback memory cache)
- OpenAI-compatible API key
- Langfuse account (optional)

## ğŸ› ï¸ Installation

### 1. Clone Repository
```bash
git clone <repository-url>
cd sentiment-agent-v2
```

### 2. Environment Setup
```bash
cp .env.example .env
# Edit .env vá»›i cÃ¡c thÃ´ng tin cáº§n thiáº¿t
```

### 3. Required Environment Variables
```bash
# LLM Configuration
OPENAI_API_KEY=your_api_key_here
LLM_MODEL=google/gemma-3-12b-it
OPENAI_URI=https://api.deepinfra.com/v1/openai

# Langfuse Tracing
LANGFUSE_SECRET_KEY=sk-lf-xxx
LANGFUSE_PUBLIC_KEY=pk-lf-xxx
LANGFUSE_HOST=http://your-langfuse-host:3002

# Performance Settings
MAX_CONCURRENT_REQUESTS=50
REQUEST_TIMEOUT=60
RATE_LIMIT=100/minute
WORKERS=4

# Cache
REDIS_URL=redis://localhost:6379
CACHE_TTL=3600
```

## ğŸš€ Deployment

### Development
```bash
# Start single instance
uvicorn app.api:app --reload --host 0.0.0.0 --port 8000

# Or with Docker
docker-compose -f docker-compose.dev.yml up
```

### Production
```bash
# Deploy vá»›i load balancing
chmod +x deploy.sh
./deploy.sh

# Hoáº·c manual
docker-compose up -d
```

## ğŸ“¡ API Usage

### Main Endpoint
```bash
POST /analyze
Content-Type: application/json

{
  "id": "unique_id",
  "index": "document_index",
  "topic": "Brand Name",
  "title": "Post title",
  "content": "Main content text",
  "description": "Additional description",
  "type": "tiktokComment",
  "main_keywords": ["brand", "product"]
}
```

### Response Format
```json
{
  "targeted": true,
  "sentiment": "positive",
  "confidence": 0.85,
  "keywords": {
    "positive": ["tá»‘t", "xuáº¥t sáº¯c"],
    "negative": []
  },
  "explanation": "NgÆ°á»i dÃ¹ng khen ngá»£i sáº£n pháº©m"
}
```

### Legacy Endpoint (Backward Compatibility)
```bash
POST /analyze/legacy
# Sá»­ dá»¥ng format cÅ©, tráº£ vá» AnalysisResult
```

## ğŸ”§ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Basic info |
| `/analyze` | POST | Main sentiment analysis |
| `/analyze/legacy` | POST | Legacy format support |
| `/health` | GET | Health check |
| `/metrics` | GET | Prometheus metrics |
| `/cache/stats` | GET | Cache statistics |
| `/cache/clear` | POST | Clear cache |

## ğŸ“Š Monitoring

### Health Check
```bash
curl http://localhost:4880/health
```

### Metrics (Prometheus)
```bash
curl http://localhost:4880/metrics
```

### Cache Statistics
```bash
curl http://localhost:4880/cache/stats
```

## ğŸ§ª Testing

### Performance Test
```bash
python3 test_production_api.py
```

### Load Test
```bash
# Sá»­ dá»¥ng Apache Bench
ab -n 1000 -c 10 -T application/json -p test_payload.json http://localhost:4880/analyze

# Hoáº·c vá»›i wrk
wrk -t12 -c400 -d30s -s test_script.lua http://localhost:4880/analyze
```

## ğŸ” Langfuse Tracing

API tá»± Ä‘á»™ng trace táº¥t cáº£ LLM calls vÃ  analysis operations:

- **Traces**: Má»—i request táº¡o má»™t trace vá»›i metadata
- **Spans**: LLM calls, text processing, caching
- **Metrics**: Response time, success rate, confidence scores
- **Debugging**: Raw LLM responses, parsing errors

Xem traces táº¡i Langfuse dashboard: `http://your-langfuse-host:3002`

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx     â”‚    â”‚   Redis     â”‚    â”‚  Langfuse   â”‚
â”‚Load Balancerâ”‚    â”‚   Cache     â”‚    â”‚  Tracing    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              API Instances (3x)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ FastAPI +   â”‚ â”‚ FastAPI +   â”‚ â”‚ FastAPI +   â”‚   â”‚
â”‚  â”‚ AI Agent    â”‚ â”‚ AI Agent    â”‚ â”‚ AI Agent    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ LLM Service â”‚
                  â”‚(DeepInfra)  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration

### Performance Tuning
```bash
# TÄƒng sá»‘ workers
WORKERS=8

# TÄƒng concurrent requests
MAX_CONCURRENT_REQUESTS=100

# TÄƒng cache TTL
CACHE_TTL=7200

# TÄƒng rate limit
RATE_LIMIT=200/minute
```

### Scaling
```bash
# Scale API instances
docker-compose up -d --scale sentiment-api-1=5

# Scale vá»›i resource limits
docker-compose up -d --scale sentiment-api-1=3 --scale sentiment-api-2=3
```

## ğŸ“ Logs

### View Logs
```bash
# All services
docker-compose logs -f

# Specific service
docker-compose logs -f sentiment-api-1

# Nginx access logs
docker-compose logs -f nginx

# Redis logs
docker-compose logs -f redis
```

### Log Locations
- Application logs: `logs/app/`
- Nginx logs: `logs/nginx/`
- Container logs: `docker-compose logs`

## ğŸš¨ Troubleshooting

### Common Issues

1. **Redis Connection Failed**
   ```bash
   # Check Redis status
   docker-compose exec redis redis-cli ping
   
   # Restart Redis
   docker-compose restart redis
   ```

2. **LLM API Errors**
   ```bash
   # Check API key
   echo $OPENAI_API_KEY
   
   # Test API directly
   curl -H "Authorization: Bearer $OPENAI_API_KEY" $OPENAI_URI/models
   ```

3. **High Memory Usage**
   ```bash
   # Check memory usage
   docker stats
   
   # Reduce cache TTL
   CACHE_TTL=1800
   ```

4. **Slow Response Times**
   ```bash
   # Check concurrent requests
   MAX_CONCURRENT_REQUESTS=30
   
   # Reduce timeout
   REQUEST_TIMEOUT=30
   ```

## ğŸ” Security

- Rate limiting enabled
- CORS configured
- Environment variables for secrets
- Health check endpoints
- Request timeout protection

## ğŸ“ˆ Performance Benchmarks

Typical performance vá»›i 3 API instances:

- **Throughput**: 200+ requests/second
- **Response Time**: 
  - Cache hit: <100ms
  - Cache miss: 1-3s (depending on LLM)
- **Concurrent Users**: 100+
- **Uptime**: 99.9%

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Add tests
4. Submit pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ†˜ Support

- GitHub Issues: [Create Issue](link-to-issues)
- Documentation: [Wiki](link-to-wiki)
- Email: support@yourcompany.com